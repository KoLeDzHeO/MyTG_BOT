import logging
import uuid
import time
from telegram import Update
from telegram.ext import ContextTypes
from telegram.constants import ChatAction
from telegram.error import BadRequest

from src.config import config
from src.gpt_client import ask_gpt
from src.utils.text_utils import chunk_text
from src.utils.ratelimit import WindowRateLimiter
from src.utils.format import as_html

_rate_limiter = WindowRateLimiter(config.RATE_LIMIT_PER_CHAT, config.RATE_LIMIT_INTERVAL)

PREFIX_MAP = {
    "..": ("ddot", "–ü–æ–ª–Ω—ã–π"),
    ".":  ("dot",  "–ö–æ—Ä–æ—Ç–∫–∏–π"),
}

def _pick_mode(text: str) -> tuple[str | None, str]:
    s = text.lstrip()
    for p, (mode, _) in PREFIX_MAP.items():
        if s.startswith(p):
            return mode, s[len(p):].strip()
    return None, s  # –Ω–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å–∞

def _model_and_tokens(mode: str) -> tuple[str, int]:
    if mode == "ddot":
        return config.MODEL_DDOT, config.MAX_TOKENS_DDOT
    return config.MODEL_DOT, config.MAX_TOKENS_DOT

async def start_handler(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    await update.message.reply_text(
        "–ü—Ä–∏–≤–µ—Ç!\n"
        "–ö–æ—Ä–æ—Ç–∫–∏–π —Ä–µ–∂–∏–º: `. –≤–æ–ø—Ä–æ—Å` (–±—ã—Å—Ç—Ä–µ–µ/–¥–µ—à–µ–≤–ª–µ).\n"
        "–ü–æ–ª–Ω—ã–π —Ä–µ–∂–∏–º: `.. –≤–æ–ø—Ä–æ—Å` (—É–º–Ω–µ–µ/–¥–ª–∏–Ω–Ω–µ–µ).\n"
        "–ö–æ–º–∞–Ω–¥—ã: /start /id\n",
    )

async def id_handler(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    await update.message.reply_text(str(update.effective_chat.id))

async def gpt_handler(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    rid = str(uuid.uuid4())[:8]
    t0 = time.time()
    try:
        chat_id = update.effective_chat.id
        text = (update.message.text or "").strip()
        if not text:
            return

        # –¢—Ä–µ–±—É–µ–º –ø—Ä–µ—Ñ–∏–∫—Å, –µ—Å–ª–∏ —Ç–∞–∫ —É–∫–∞–∑–∞–Ω–æ –≤ –∫–æ–Ω—Ñ–∏–≥–µ
        mode, clean_text = _pick_mode(text)
        if config.REQUIRE_PREFIX and mode is None:
            return  # –∏–≥–Ω–æ—Ä –±–µ–∑ –æ—Ç–≤–µ—Ç–∞

        # –ï—Å–ª–∏ –ø—Ä–µ—Ñ–∏–∫—Å –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω –∏ –µ–≥–æ –Ω–µ—Ç ‚Äî –¥–µ–ª–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ä–µ–∂–∏–º
        if mode is None:
            mode = "dot"

        if not _rate_limiter.allow(chat_id):
            await update.message.reply_text("–°–ª–∏—à–∫–æ–º —á–∞—Å—Ç–æ. –ü–æ–ø—Ä–æ–±—É–π —á–µ—Ä–µ–∑ –º–∏–Ω—É—Ç–∫—É üôè")
            return

        if not clean_text:
            await update.message.reply_text("–î–æ–±–∞–≤—å —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ –ø—Ä–µ—Ñ–∏–∫—Å–∞ `. –∏–ª–∏ ..`")
            return

        if len(clean_text) > config.MAX_PROMPT_CHARS:
            await update.message.reply_text(f"–°–æ–∫—Ä–∞—Ç–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ (>{config.MAX_PROMPT_CHARS} —Å–∏–º–≤–æ–ª–æ–≤).")
            return

        model, max_tokens = _model_and_tokens(mode)

        await context.bot.send_chat_action(chat_id=chat_id, action=ChatAction.TYPING)

        system = (
            "–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É. "
            "–°–æ—Ö—Ä–∞–Ω—è–π —è–∑—ã–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (—Ä—É—Å—Å–∫–∏–π/–ø–æ–ª—å—Å–∫–∏–π). "
            "–ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –¥–≤—É—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π ‚Äî —É—Ç–æ—á–Ω—è–π –ª–∞–∫–æ–Ω–∏—á–Ω–æ."
        )

        logging.info("rid=%s start model=%s prompt_len=%d", rid, model, len(clean_text))
        answer = await ask_gpt(
            api_key=config.OPENAI_API_KEY,
            model=model,
            system=system,
            prompt=clean_text,
            max_tokens=max_tokens,
            timeout=30,  # <= –≤–µ—Ä–Ω—É–ª–∏ 30 —Å–µ–∫
        )

        if not answer:
            await update.message.reply_text("–°–µ—Ä–≤–∏—Å –∑–∞–Ω—è—Ç –∏–ª–∏ —Ç–∏—à–∏–Ω–∞ –æ—Ç –º–æ–¥–µ–ª–∏. –ü–æ–ø—Ä–æ–±—É–π –µ—â—ë —Ä–∞–∑ –ø–æ–∑–∂–µ.")
            return

        chunks = chunk_text(answer, config.MAX_REPLY_CHARS)
        if not chunks:
            chunks = [answer or ""]  # —Å—Ç—Ä–∞—Ö–æ–≤–∫–∞
        try:
            msg = await update.message.reply_text(as_html(chunks[0]), parse_mode="HTML")
        except BadRequest:
            msg = await update.message.reply_text(chunks[0])  # plain-text fallback
        except Exception:
            try:
                msg = await update.message.reply_text(chunks[0])
            except Exception:
                msg = None

        for ch in chunks[1:]:
            if msg is None:
                # –µ—Å–ª–∏ –ø–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–µ –æ—Ç–ø—Ä–∞–≤–∏–ª–æ—Å—å ‚Äî —Å—Ä–∞–∑—É —à–ª—ë–º –Ω–æ–≤—ã–µ —á–∞–Ω–∫–∏
                await update.message.reply_text(ch)
                continue
            try:
                await msg.edit_text(as_html(ch), parse_mode="HTML")
            except BadRequest:
                await update.message.reply_text(ch)  # plain-text fallback
            except Exception:
                try:
                    await update.message.reply_text(ch)
                except Exception:
                    pass
        logging.info(
            "rid=%s done in=%.2fs answer_len=%d chunks=%d",
            rid,
            time.time() - t0,
            len(answer),
            len(chunks),
        )

    except Exception as e:
        logging.exception("rid=%s gpt_handler error: %s", rid, e)
        try:
            await update.message.reply_text("‚ö†Ô∏è –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π –µ—â—ë —Ä–∞–∑, —è —É–∂–µ —Å–º–æ—Ç—Ä—é –ª–æ–≥–∏.")
        except Exception:
            pass
